{
  "model_path": "C:\\LLM\\gguf",
  "gpu_layers": -1,
  "inference_cancel_key": 27,
  "models": [
    {
      "filename": "Phi-3-mini-4k-instruct-q4.gguf",
      "name": "phi3",
      "max_context": 4000,
      "template": "<|%s|>%s<|im_end|>",
      "template_end": "<|assistant|>",
      "stop": [
        "<|user|>",
        "<|assistant|>",
        "<|system|>",
        "<|end|>",
        "<|endoftext|>"
      ]
    },
    {
      "filename": "Meta-Llama-3-8B-Instruct-Q6_K.gguf",
      "name": "llama3",
      "max_context": 8000,
      "template": "<|begin_of_text|><|start_header_id|>%s<|end_header_id|>%s<|eot_id|>",
      "template_end": "<|start_header_id|>assistant<|end_header_id|>",
      "stop": [
        "<|eot_id|>",
        "<|start_header_id|>",
        "<|end_header_id|>",
        "assistant"
      ]
    }
  ]
}